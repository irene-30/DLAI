"""
Contains PyTorch Dataset classes for loading data.
"""
import torch
from torch.utils.data import Dataset
from transformers import PreTrainedTokenizer
from datasets import load_dataset
from typing import List, Dict
from src.utils import parse_gsm8k_sample

class VQVAE_Dataset(Dataset):
    """
    Custom Dataset for Stage 1 (Training VQ-VAE).
    Loads raw data and tokenizes the full (P+C+S) sequence.
    """
    def __init__(self, tokenizer: PreTrainedTokenizer, data: List[Dict], max_length: int):
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.samples = []
        
        for sample in data:
            parsed = parse_gsm8k_sample(sample)
            if not parsed:
                continue
            
            # Train VQ-VAE on the whole sequence X = P + C + S
            full_text = parsed[0] + parsed[1] + parsed[2]
            
            # Tokenize and store
            tokenized = self.tokenizer(
                full_text,
                max_length=self.max_length,
                padding="max_length",
                truncation=True,
                return_tensors="pt"
            )
            self.samples.append(tokenized)

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        # Return squeezed tensors
        input_ids = self.samples[idx]['input_ids'].squeeze(0)
        attention_mask = self.samples[idx]['attention_mask'].squeeze(0)
        return {"input_ids": input_ids, "attention_mask": attention_mask}


class AssortedDataset(Dataset):
    """
    Custom Dataset for Stage 2 (Training LLM).
    Loads the *pre-processed* assorted dataset.
    """
    def __init__(self, tokenizer: PreTrainedTokenizer, file_path: str, max_length: int):
        self.tokenizer = tokenizer
        self.max_length = max_length
        print(f"Loading assorted dataset from {file_path}...")
        # Load the .jsonl file generated by the preprocessing notebook
        self.data = load_dataset("json", data_files=file_path, split="train")

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        # Get the text (P + C_assorted + S)
        text = self.data[idx]['text']
        
        tokenized = self.tokenizer(
            text,
            max_length=self.max_length,
            padding="max_length",
            truncation=True,
        )
        
        # Create labels for Causal LM (labels are the same as input_ids)
        tokenized["labels"] = tokenized["input_ids"].copy()
        
        return tokenized