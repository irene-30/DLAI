{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Evaluation, Inference, and Interpretation\n",
    "\n",
    "**Objective:** Test our fine-tuned LLM. We will:\n",
    "1.  Run the formal evaluation script (`src/evaluate.py`) to get an accuracy score on the test set.\n",
    "2.  Run live inference on a custom question.\n",
    "3.  **Interpret** the generated latent tokens by decoding them with our VQ-VAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install datasets transformers torch accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import re\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "# Add 'src' to path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "from src.utils import (\n",
    "    get_llm_tokenizer, MAX_SEQ_LEN, VQ_CODEBOOK_SIZE, \n",
    "    PATH_LLM_MODEL, PATH_VQVAE_MODEL\n",
    ")\n",
    "from src.model.vae import VQVAEModel\n",
    "from src.evaluate import evaluate_model\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Run Evaluation\n",
    "\n",
    "This will load our fine-tuned LLM, run it on the *entire* GSM8K test set, and report the final accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is imported from src/evaluate.py\n",
    "evaluate_model(model_path=PATH_LLM_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Live Inference\n",
    "\n",
    "Let's ask our model a new question. We'll print the raw output so we can see the latent tokens it generates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load fine-tuned LLM and tokenizer\n",
    "llm_tokenizer = get_llm_tokenizer()\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(PATH_LLM_MODEL).to(device)\n",
    "llm_model.eval()\n",
    "\n",
    "# 2. Define a question\n",
    "question = \"Mark has $50. He buys 3 books that cost $7 each. How much money does he have left?\"\n",
    "prompt = f\"Question: {question}\\nAnswer: \"\n",
    "\n",
    "# 3. Generate a response\n",
    "inputs = llm_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "with torch.no_grad():\n",
    "    output = llm_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=150,\n",
    "        pad_token_id=llm_tokenizer.pad_token_id,\n",
    "        eos_token_id=llm_tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "generated_text = llm_tokenizer.decode(output[0], skip_special_tokens=False)\n",
    "\n",
    "print(\"--- GENERATED RESPONSE ---\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Interpretation of Latent Tokens\n",
    "\n",
    "This is the most interesting part. We can take the `<latent_...>` tokens generated by our LLM and feed them to the *decoder* of our VQ-VAE to see what reasoning they represent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load the trained VQ-VAE\n",
    "vq_model = VQVAEModel(\n",
    "    vocab_size=len(llm_tokenizer),\n",
    "    d_model=256, # Must match d_model from notebook 02\n",
    "    num_embeddings=VQ_CODEBOOK_SIZE,\n",
    "    max_seq_len=MAX_SEQ_LEN\n",
    ").to(device)\n",
    "\n",
    "try:\n",
    "    vq_model.load_state_dict(torch.load(PATH_VQVAE_MODEL, map_location=device))\n",
    "    vq_model.eval()\n",
    "    print(f\"Loaded VQ-VAE for interpretation.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Could not load VQ-VAE model. Skipping interpretation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Find all latent tokens in the generated text\n",
    "latent_token_ids = [int(i) for i in re.findall(r\"<latent_(\\d+)>\", generated_text)]\n",
    "\n",
    "if not latent_token_ids:\n",
    "    print(\"No latent tokens were generated in the response.\")\n",
    "else:\n",
    "    print(f\"Found {len(latent_token_ids)} latent tokens: {latent_token_ids}\")\n",
    "    \n",
    "    # 3. Get the corresponding embeddings from the VQ codebook\n",
    "    indices_tensor = torch.tensor(latent_token_ids, dtype=torch.long).to(device)\n",
    "    codebook_embeddings = vq_model.quantizer.embedding(indices_tensor)\n",
    "    \n",
    "    # 4. Decode them!\n",
    "    # Unsqueeze to add batch dim: (T, D) -> (1, T, D)\n",
    "    quantized_memory = codebook_embeddings.unsqueeze(0)\n",
    "    \n",
    "    # For a simple autoencoder, we feed dummy tokens to the decoder\n",
    "    # A better approach would be to feed the prompt tokens as well\n",
    "    start_token_id = llm_tokenizer.bos_token_id if llm_tokenizer.bos_token_id else 0\n",
    "    decoder_input_ids = torch.full((1, len(latent_token_ids)), start_token_id, dtype=torch.long).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = vq_model.decode(quantized_memory, decoder_input_ids)\n",
    "        \n",
    "    # Get the most likely token ID for each position\n",
    "    predicted_token_ids = torch.argmax(logits, dim=-1).squeeze(0)\n",
    "    \n",
    "    # 5. Decode the token IDs back to text\n",
    "    interpreted_text = llm_tokenizer.decode(predicted_token_ids, skip_special_tokens=True)\n",
    "    \n",
    "    print(\"\\n--- INTERPRETATION OF LATENT THOUGHTS ---\")\n",
    "    print(interpreted_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}