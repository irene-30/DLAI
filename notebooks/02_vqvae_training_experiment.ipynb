{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. VQ-VAE Training (Stage 1)\n",
    "\n",
    "**Objective:** Train the VQ-VAE model from `src.model.vae` on the full `P+C+S` sequences. We will run a small training loop here, plot the losses, and save the final model weights to `experiments/vqvae_stage1.pth`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install datasets transformers torch tqdm matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Add 'src' to path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "from src.utils import (\n",
    "    get_llm_tokenizer, MAX_SEQ_LEN, PATH_VQVAE_MODEL,\n",
    "    VQ_CODEBOOK_SIZE\n",
    ")\n",
    "from src.dataset import VQVAE_Dataset\n",
    "from src.model.vae import VQVAEModel\n",
    "\n",
    "# --- Configuration ---\n",
    "D_MODEL = 256\n",
    "NUM_EPOCHS = 3 # Increase for a real run\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 1e-4\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Load Tokenizer and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_llm_tokenizer()\n",
    "vocab_size = len(tokenizer)\n",
    "\n",
    "print(f\"Tokenizer vocabulary size (including new tokens): {vocab_size}\")\n",
    "\n",
    "raw_dataset = load_dataset(\"gsm8k\", \"main\")['train']\n",
    "train_dataset = VQVAE_Dataset(tokenizer, raw_dataset, max_length=MAX_SEQ_LEN)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "print(f\"Loaded {len(train_dataset)} samples for VQ-VAE training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Initialize Model and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VQVAEModel(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=D_MODEL,\n",
    "    num_embeddings=VQ_CODEBOOK_SIZE,\n",
    "    max_seq_len=MAX_SEQ_LEN\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print(f\"VQ-VAE Model parameters: {sum(p.numel() for p in model.parameters())/1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Training Loop\n",
    "\n",
    "We'll run the training loop directly in the notebook to monitor its progress and plot the losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "losses = []\n",
    "recon_losses = []\n",
    "vq_losses = []\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"--- EPOCH {epoch+1}/{NUM_EPOCHS} ---\")\n",
    "    epoch_loss, epoch_recon, epoch_vq = 0, 0, 0\n",
    "    \n",
    "    for batch in tqdm(train_loader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        total_loss, recon_loss, vq_loss = model(input_ids)\n",
    "        \n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += total_loss.item()\n",
    "        epoch_recon += recon_loss.item()\n",
    "        epoch_vq += vq_loss.item()\n",
    "    \n",
    "    # Log average losses for the epoch\n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    avg_recon = epoch_recon / len(train_loader)\n",
    "    avg_vq = epoch_vq / len(train_loader)\n",
    "    \n",
    "    losses.append(avg_loss)\n",
    "    recon_losses.append(avg_recon)\n",
    "    vq_losses.append(avg_vq)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1} | Avg Loss: {avg_loss:.4f} | Recon: {avg_recon:.4f} | VQ: {avg_vq:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Visualize Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(losses, label='Total Loss')\n",
    "plt.plot(recon_losses, label='Reconstruction Loss', linestyle='--')\n",
    "plt.plot(vq_losses, label='VQ Loss', linestyle=':')\n",
    "plt.title('VQ-VAE Training Losses')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Save the Model\n",
    "\n",
    "Finally, we save the trained VQ-VAE weights. These will be loaded by the next notebook to create the assorted dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Saving VQ-VAE model to {PATH_VQVAE_MODEL}\")\n",
    "os.makedirs(os.path.dirname(PATH_VQVAE_MODEL), exist_ok=True)\n",
    "torch.save(model.state_dict(), PATH_VQVAE_MODEL)\n",
    "print(\"Model saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}