{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Preprocessing (Stage 2) - Create Assorted Dataset\n",
    "\n",
    "**Objective:** Load the trained VQ-VAE (from Stage 1) and use it to process the raw GSM8K dataset. This will create the `assorted_train.jsonl` file, which contains the `P + C_assorted + S` sequences for training our main LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install datasets transformers torch tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Add 'src' to path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "from src.utils import (\n",
    "    get_llm_tokenizer, MAX_SEQ_LEN, PATH_VQVAE_MODEL,\n",
    "    VQ_CODEBOOK_SIZE, PATH_PROCESSED_DATA,\n",
    "    create_assorted_dataset\n",
    ")\n",
    "from src.model.vae import VQVAEModel\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Load Tokenizer and Trained VQ-VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_llm_tokenizer()\n",
    "vocab_size = len(tokenizer)\n",
    "\n",
    "# 1. Instantiate the VQ-VAE model structure\n",
    "# Note: The parameters (d_model, etc.) MUST match those used in notebook 02.\n",
    "vq_model = VQVAEModel(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=256, # Must match d_model from notebook 02\n",
    "    num_embeddings=VQ_CODEBOOK_SIZE,\n",
    "    max_seq_len=MAX_SEQ_LEN\n",
    ").to(device)\n",
    "\n",
    "# 2. Load the trained weights\n",
    "try:\n",
    "    vq_model.load_state_dict(torch.load(PATH_VQVAE_MODEL, map_location=device))\n",
    "    vq_model.eval()\n",
    "    print(f\"Successfully loaded trained VQ-VAE from {PATH_VQVAE_MODEL}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: VQ-VAE model not found at {PATH_VQVAE_MODEL}\")\n",
    "    print(\"Please run '02_vqvae_training_experiment.ipynb' first.\")\n",
    "    # This notebook will fail if the model isn't trained, which is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Load Raw Data and Create Assorted Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = load_dataset(\"gsm8k\", \"main\")['train']\n",
    "\n",
    "# This function does all the heavy lifting:\n",
    "# 1. Encodes CoT with VQ-VAE\n",
    "# 2. Applies randomized replacement\n",
    "# 3. Creates the final text string\n",
    "assorted_samples = create_assorted_dataset(\n",
    "    vq_model=vq_model,\n",
    "    llm_tokenizer=tokenizer,\n",
    "    dataset=raw_dataset\n",
    ")\n",
    "\n",
    "print(f\"\\nGenerated {len(assorted_samples)} assorted samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Inspect Assorted Samples\n",
    "\n",
    "Let's look at a few samples. Some should be 100% text (when `m=0`) and others should be mixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- SAMPLE 1 ---\")\n",
    "print(assorted_samples[0]['text'])\n",
    "\n",
    "print(\"\\n--- SAMPLE 2 ---\")\n",
    "print(assorted_samples[1]['text'])\n",
    "\n",
    "print(\"\\n--- SAMPLE 3 ---\")\n",
    "print(assorted_samples[2]['text'])\n",
    "\n",
    "# Find a sample that likely has latent tokens\n",
    "latent_sample = next((s['text'] for s in assorted_samples if \"[boLatent]\" in s['text']), \"No latent sample found in batch.\")\n",
    "print(\"\\n--- SAMPLE WITH LATENT TOKENS ---\")\n",
    "print(latent_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Save Processed Data\n",
    "\n",
    "Finally, we save this list of dictionaries as a `.jsonl` file. This will be read by the LLM training notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Saving processed data to {PATH_PROCESSED_DATA}...\")\n",
    "os.makedirs(os.path.dirname(PATH_PROCESSED_DATA), exist_ok=True)\n",
    "\n",
    "with open(PATH_PROCESSED_DATA, 'w') as f:\n",
    "    for item in assorted_samples:\n",
    "        f.write(json.dumps(item) + '\\n')\n",
    "\n",
    "print(\"Processed data saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}