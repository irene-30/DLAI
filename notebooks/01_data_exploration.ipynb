{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Exploration & Visualization (GSM8K)\n",
    "\n",
    "**Objective:** Load the raw GSM8K dataset, parse it into our `(Prompt, CoT, Solution)` format, and visualize its properties. This will help us understand the data before we build any models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install datasets transformers matplotlib seaborn pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the current working directory (which should be the project root)\n",
    "project_root = os.path.abspath(os.getcwd())\n",
    "# If 'src' is not in the current directory, we're probably in 'notebooks'\n",
    "if 'src' not in os.listdir(project_root):\n",
    "    # Go up one level to the actual project root\n",
    "    project_root = os.path.abspath(os.path.join(project_root, '..'))\n",
    "    # Add the project root to the Python path\n",
    "    if project_root not in sys.path:\n",
    "        sys.path.insert(0, project_root)\n",
    "        print(f\\\"Added project root to path: {project_root}\\\")\n",
    "# Add the 'src' directory to the Python path\n",
    "#sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "# Now we can import from our source files\n",
    "from src.utils import parse_gsm8k_sample\n",
    "from src.utils import get_llm_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Load Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"gsm8k\", \"main\")\n",
    "train_data = dataset['train']\n",
    "test_data = dataset['test']\n",
    "\n",
    "print(f\"Training samples: {len(train_data)}\")\n",
    "print(f\"Test samples: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Inspect a Single Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = train_data[0]\n",
    "print(\"--- RAW QUESTION ---\")\n",
    "print(sample['question'])\n",
    "\n",
    "print(\"\\n--- RAW ANSWER (CoT + Solution) ---\")\n",
    "print(sample['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Parse the Sample\n",
    "\n",
    "Let's test our utility function `parse_gsm8k_sample` from `src/utils.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed = parse_gsm8k_sample(sample)\n",
    "\n",
    "if parsed:\n",
    "    prompt, cot, solution = parsed\n",
    "    print(\"--- PARSED PROMPT (P) ---\")\n",
    "    print(f\"{prompt!r}\")\n",
    "    \n",
    "    print(\"\\n--- PARSED CoT (C) ---\")\n",
    "    print(f\"{cot!r}\")\n",
    "    \n",
    "    print(\"\\n--- PARSED SOLUTION (S) ---\")\n",
    "    print(f\"{solution!r}\")\n",
    "else:\n",
    "    print(\"Failed to parse sample.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Visualize Token Lengths\n",
    "\n",
    "This is the most important visualization. It will show us how long the **Chain-of-Thought (C)** sequences are. If they are long, it justifies our paper's approach of compressing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Load our tokenizer to get accurate lengths\n",
    "tokenizer = get_llm_tokenizer()\n",
    "\n",
    "lengths = []\n",
    "for sample in train_data:\n",
    "    parsed = parse_gsm8k_sample(sample)\n",
    "    if parsed:\n",
    "        prompt, cot, solution = parsed\n",
    "        lengths.append({\n",
    "            'prompt_len': len(tokenizer.encode(prompt)),\n",
    "            'cot_len': len(tokenizer.encode(cot)),\n",
    "            'solution_len': len(tokenizer.encode(solution)),\n",
    "            'full_text_len': len(tokenizer.encode(prompt + cot + solution))\n",
    "        })\n",
    "\n",
    "df_lengths = pd.DataFrame(lengths)\n",
    "df_lengths.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "sns.histplot(df_lengths['cot_len'], bins=50, ax=ax1, kde=True)\n",
    "ax1.set_title('Distribution of Chain-of-Thought (CoT) Token Lengths')\n",
    "ax1.set_xlabel('Token Length')\n",
    "\n",
    "sns.histplot(df_lengths['full_text_len'], bins=50, ax=ax2, kde=True)\n",
    "ax2.set_title('Distribution of Full Sample (P+C+S) Token Lengths')\n",
    "ax2.set_xlabel('Token Length')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"--- Analysis ---\")\n",
    "print(f\"Average CoT length: {df_lengths['cot_len'].mean():.2f} tokens\")\n",
    "print(f\"Max CoT length: {df_lengths['cot_len'].max()} tokens\")\n",
    "print(\"Conclusion: The CoT sequences are often long, making them a good target for compression.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
